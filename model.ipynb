{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2pDjB7sQ4sC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import random\n",
        "\n",
        "random.seed(250)\n",
        "\n",
        "\n",
        "class Connection:\n",
        "    def __init__(self, weight):\n",
        "        self.weight = weight\n",
        "        self.delta_weight = 0.0\n",
        "\n",
        "\n",
        "class Neuron:\n",
        "    # Class constants\n",
        "    eta = 0.15  # Overall net learning rate [0.0, 1.0]\n",
        "    alpha = 0.5  # Multiplier of last weight change, momentum [0.0, 1.0]\n",
        "\n",
        "    def __init__(self, my_idx, num_outputs):\n",
        "        \"\"\"Constructor for Neuron\n",
        "        :param num_outputs: The number of out-links this neuron has\n",
        "        \"\"\"\n",
        "        self.output_val = 1.0\n",
        "        self.my_idx = my_idx\n",
        "        self.gradient = 0.0\n",
        "        # Each element in output weights is a Connection object\n",
        "        self.output_weights = list()\n",
        "        for i in range(num_outputs):\n",
        "            self.output_weights.append(Connection(self.random_weight()))\n",
        "\n",
        "    @staticmethod\n",
        "    def random_weight():\n",
        "        return random.random()\n",
        "\n",
        "    @staticmethod\n",
        "    def transfer_function(weighted_sum):\n",
        "        \"\"\"Performs activation function on the weighted_sum\n",
        "        :param weighted_sum: The weighted sum from the previous layer\n",
        "        :return: Transformed weighted sum\n",
        "        \"\"\"\n",
        "        # Performing tanh on the weighted sum. Range (-1.0, 1.0)\n",
        "        return math.tanh(weighted_sum)\n",
        "\n",
        "    @staticmethod\n",
        "    def transfer_function_derivative(weighted_sum):\n",
        "        \"\"\"Performs derivative of activation function on the weighted_sum\n",
        "        :param weighted_sum: The weighted sum from the previous layer\n",
        "        :return: Transformed weighted sum\n",
        "        \"\"\"\n",
        "        # derivative of tanh(x) is (1 - x^2)\n",
        "        return 1.0 - (weighted_sum ** 2)\n",
        "\n",
        "    def feed_forward(self, prev_layer):\n",
        "        \"\"\"Performs forward propagation by computing output value of a neuron\n",
        "        :param prev_layer: List of previous layer neurons\n",
        "        \"\"\"\n",
        "        weighted_sum = 0.0\n",
        "\n",
        "        for prev_neuron in prev_layer:\n",
        "            weighted_sum += prev_neuron.output_val * prev_neuron.output_weights[self.my_idx].weight\n",
        "\n",
        "        self.output_val = self.transfer_function(weighted_sum)\n",
        "\n",
        "    def calc_output_gradient(self, target_val):\n",
        "        \"\"\"Computes the target values for the output neuron\n",
        "        :param target_val:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        delta = target_val - self.output_val\n",
        "        self.gradient = delta * self.transfer_function_derivative(self.output_val)\n",
        "\n",
        "    def sum_dow(self, next_layer):\n",
        "        \"\"\"\n",
        "        :param next_layer:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        dow_sum = 0.0\n",
        "        # Sum our contributions of the errors at the nodes we feed\n",
        "        for neuron_idx in range(len(next_layer) - 1):\n",
        "            dow_sum += (self.output_weights[neuron_idx].weight * next_layer[neuron_idx].gradient)\n",
        "        return dow_sum\n",
        "\n",
        "    def calc_hidden_gradient(self, next_layer):\n",
        "        \"\"\"\n",
        "        :param next_layer:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        dow = self.sum_dow(next_layer)\n",
        "        self.gradient = dow * self.transfer_function_derivative(self.output_val)\n",
        "\n",
        "    def update_input_weights(self, prev_layer):\n",
        "        \"\"\"This method is called after back propagation to update the input weights\n",
        "        :param prev_layer: List of previous layer neurons\n",
        "        \"\"\"\n",
        "        # The weights to be updated are in the connection container\n",
        "        # in the neurons in the preceding layer\n",
        "        for neuron in prev_layer:\n",
        "            old_delta_weight = neuron.output_weights[self.my_idx].delta_weight\n",
        "            new_delta_weight = (neuron.eta * neuron.output_val * self.gradient) + (neuron.alpha * old_delta_weight)\n",
        "            neuron.output_weights[self.my_idx].delta_weight = new_delta_weight\n",
        "            neuron.output_weights[self.my_idx].weight += new_delta_weight\n",
        "\n",
        "    def __str__(self):\n",
        "        \"\"\"Returns the string representation of the object for printing\n",
        "        :return: The string representation of the object\n",
        "        \"\"\"\n",
        "        ret_str = 'My index:' + str(self.my_idx) + '\\nOutput weights:'\n",
        "        for conn in self.output_weights:\n",
        "            ret_str += str(conn.weight) + ' '\n",
        "        return ret_str\n",
        "\n",
        "\n",
        "class Net:\n",
        "    def __init__(self, topology):\n",
        "        \"\"\"Constructor to create a neural net\n",
        "        :param topology: List that contains the number of neurons in each layer\n",
        "        \"\"\"\n",
        "        # Represent the layers in the neural net\n",
        "        self.layers = list()\n",
        "\n",
        "        # Each layer contains a list of neurons\n",
        "        for layer_num, neuron_count in enumerate(topology):\n",
        "            neuron_list = list()\n",
        "            num_outputs = 0 if layer_num == len(topology) - 1 else topology[layer_num + 1]\n",
        "            for index in range(neuron_count + 1):\n",
        "                neuron_list.append(Neuron(index, num_outputs))\n",
        "            self.layers.append(neuron_list)\n",
        "\n",
        "        self.error = 0.0\n",
        "\n",
        "    def feed_forward(self, input_vals):\n",
        "        \"\"\"Performs forward propagation in the network\n",
        "        :param input_vals: List of input parameters\n",
        "        \"\"\"\n",
        "        assert len(input_vals) == len(self.layers[0]) - 1\n",
        "\n",
        "        # Assigning input values to the input neurons\n",
        "        for index in range(len(input_vals)):\n",
        "            self.layers[0][index].output_val = input_vals[index]\n",
        "\n",
        "        # Forward propagation\n",
        "        for index in range(1, len(self.layers)):\n",
        "            prev_layer = self.layers[index - 1]\n",
        "            for neuron_idx in range(len(self.layers[index]) - 1):\n",
        "                self.layers[index][neuron_idx].feed_forward(prev_layer)\n",
        "\n",
        "    def back_prop(self, target_vals):\n",
        "        \"\"\"Performs back propagation in the network\n",
        "        :param target_vals: List of output values\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # Calculate overall net error (RMS of output neuron errors)\n",
        "        output_layer = self.layers[len(self.layers) - 1]\n",
        "        error = 0.0\n",
        "\n",
        "        for neuron_idx in range(len(target_vals)):\n",
        "            delta = target_vals[neuron_idx] - output_layer[neuron_idx].output_val\n",
        "            error += (delta ** 2)\n",
        "\n",
        "        error /= (len(target_vals))  # Average error squared\n",
        "        self.error = math.sqrt(error)  # RMS\n",
        "\n",
        "        # Calculate output layer gradients\n",
        "        for neuron_idx in range(0, len(output_layer) - 1):\n",
        "            output_layer[neuron_idx].calc_output_gradient(target_vals[neuron_idx])\n",
        "\n",
        "        # Calculate gradients on hidden layers\n",
        "        for layer_idx in range(len(self.layers) - 2, 0, -1):\n",
        "            hidden_layer = self.layers[layer_idx]\n",
        "            next_layer = self.layers[layer_idx + 1]\n",
        "\n",
        "            for hidden_neuron in hidden_layer:\n",
        "                hidden_neuron.calc_hidden_gradient(next_layer)\n",
        "\n",
        "        # For all layers from output to first hidden layer,\n",
        "        # update connection weights\n",
        "        for layer_idx in range(len(self.layers) - 1, 0, -1):\n",
        "            curr_layer = self.layers[layer_idx]\n",
        "            prev_layer = self.layers[layer_idx - 1]\n",
        "\n",
        "            for neuron_idx in range(0, len(curr_layer) - 1):\n",
        "                neuron = curr_layer[neuron_idx]\n",
        "                neuron.update_input_weights(prev_layer)\n",
        "\n",
        "    def get_results(self):\n",
        "        \"\"\"Returns the results\n",
        "        :return: An array of predicted output\n",
        "        \"\"\"\n",
        "        results = list()\n",
        "        output_layer = self.layers[len(self.layers) - 1]\n",
        "        for idx in range(0, len(output_layer) - 1):\n",
        "            results.append(output_layer[idx].output_val)\n",
        "        return results\n",
        "\n",
        "\n",
        "def build_model(input_shape):\n",
        "    \"\"\"\n",
        "    Arugments:\n",
        "        input_shape: A tuple representing the shape of the input of the model. shape=(image_width, image_height, #_channels)\n",
        "    Returns:\n",
        "        model: A Model object.\n",
        "    \"\"\"\n",
        "    # Define the input placeholder as a tensor with shape input_shape. \n",
        "    X_input = Input(input_shape) # shape=(?, 240, 240, 3)\n",
        "    \n",
        "    # Zero-Padding: pads the border of X_input with zeroes\n",
        "    X = ZeroPadding2D((2, 2))(X_input) # shape=(?, 244, 244, 3)\n",
        "    \n",
        "    # CONV -> BN -> RELU Block applied to X\n",
        "    X = Conv2D(32, (7, 7), strides = (1, 1), name = 'conv0')(X)\n",
        "    X = BatchNormalization(axis = 3, name = 'bn0')(X)\n",
        "    X = Activation('relu')(X) # shape=(?, 238, 238, 32)\n",
        "    \n",
        "    # MAXPOOL\n",
        "    X = MaxPooling2D((4, 4), name='max_pool0')(X) # shape=(?, 59, 59, 32) \n",
        "    \n",
        "    # MAXPOOL\n",
        "    X = MaxPooling2D((4, 4), name='max_pool1')(X) # shape=(?, 14, 14, 32)\n",
        "    \n",
        "    # FLATTEN X \n",
        "    X = Flatten()(X) # shape=(?, 6272)\n",
        "    # FULLYCONNECTED\n",
        "    X = Dense(1, activation='sigmoid', name='fc')(X) # shape=(?, 1)\n",
        "    \n",
        "    # Create model. This creates your Keras model instance, you'll use this instance to train/test the model.\n",
        "    model = Model(inputs = X_input, outputs = X, name='BrainDetectionModel')\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}