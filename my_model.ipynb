{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "my_model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOpdjo5NdM8FWJShl4Srdw9"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2pDjB7sQ4sC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import random\n",
        "\n",
        "random.seed(250)\n",
        "\n",
        "\n",
        "class Connection:\n",
        "    def __init__(self, weight):\n",
        "        self.weight = weight\n",
        "        self.delta_weight = 0.0\n",
        "\n",
        "\n",
        "class Neuron:\n",
        "    # Class constants\n",
        "    eta = 0.15  # Overall net learning rate [0.0, 1.0]\n",
        "    alpha = 0.5  # Multiplier of last weight change, momentum [0.0, 1.0]\n",
        "\n",
        "    def __init__(self, my_idx, num_outputs):\n",
        "        \"\"\"Constructor for Neuron\n",
        "        :param num_outputs: The number of out-links this neuron has\n",
        "        \"\"\"\n",
        "        self.output_val = 1.0\n",
        "        self.my_idx = my_idx\n",
        "        self.gradient = 0.0\n",
        "        # Each element in output weights is a Connection object\n",
        "        self.output_weights = list()\n",
        "        for i in range(num_outputs):\n",
        "            self.output_weights.append(Connection(self.random_weight()))\n",
        "\n",
        "    @staticmethod\n",
        "    def random_weight():\n",
        "        return random.random()\n",
        "\n",
        "    @staticmethod\n",
        "    def transfer_function(weighted_sum):\n",
        "        \"\"\"Performs activation function on the weighted_sum\n",
        "        :param weighted_sum: The weighted sum from the previous layer\n",
        "        :return: Transformed weighted sum\n",
        "        \"\"\"\n",
        "        # Performing tanh on the weighted sum. Range (-1.0, 1.0)\n",
        "        return math.tanh(weighted_sum)\n",
        "\n",
        "    @staticmethod\n",
        "    def transfer_function_derivative(weighted_sum):\n",
        "        \"\"\"Performs derivative of activation function on the weighted_sum\n",
        "        :param weighted_sum: The weighted sum from the previous layer\n",
        "        :return: Transformed weighted sum\n",
        "        \"\"\"\n",
        "        # derivative of tanh(x) is (1 - x^2)\n",
        "        return 1.0 - (weighted_sum ** 2)\n",
        "\n",
        "    def feed_forward(self, prev_layer):\n",
        "        \"\"\"Performs forward propagation by computing output value of a neuron\n",
        "        :param prev_layer: List of previous layer neurons\n",
        "        \"\"\"\n",
        "        weighted_sum = 0.0\n",
        "\n",
        "        for prev_neuron in prev_layer:\n",
        "            weighted_sum += prev_neuron.output_val * prev_neuron.output_weights[self.my_idx].weight\n",
        "\n",
        "        self.output_val = self.transfer_function(weighted_sum)\n",
        "\n",
        "    def calc_output_gradient(self, target_val):\n",
        "        \"\"\"Computes the target values for the output neuron\n",
        "        :param target_val:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        delta = target_val - self.output_val\n",
        "        self.gradient = delta * self.transfer_function_derivative(self.output_val)\n",
        "\n",
        "    def sum_dow(self, next_layer):\n",
        "        \"\"\"\n",
        "        :param next_layer:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        dow_sum = 0.0\n",
        "        # Sum our contributions of the errors at the nodes we feed\n",
        "        for neuron_idx in range(len(next_layer) - 1):\n",
        "            dow_sum += (self.output_weights[neuron_idx].weight * next_layer[neuron_idx].gradient)\n",
        "        return dow_sum\n",
        "\n",
        "    def calc_hidden_gradient(self, next_layer):\n",
        "        \"\"\"\n",
        "        :param next_layer:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        dow = self.sum_dow(next_layer)\n",
        "        self.gradient = dow * self.transfer_function_derivative(self.output_val)\n",
        "\n",
        "    def update_input_weights(self, prev_layer):\n",
        "        \"\"\"This method is called after back propagation to update the input weights\n",
        "        :param prev_layer: List of previous layer neurons\n",
        "        \"\"\"\n",
        "        # The weights to be updated are in the connection container\n",
        "        # in the neurons in the preceding layer\n",
        "        for neuron in prev_layer:\n",
        "            old_delta_weight = neuron.output_weights[self.my_idx].delta_weight\n",
        "            new_delta_weight = (neuron.eta * neuron.output_val * self.gradient) + (neuron.alpha * old_delta_weight)\n",
        "            neuron.output_weights[self.my_idx].delta_weight = new_delta_weight\n",
        "            neuron.output_weights[self.my_idx].weight += new_delta_weight\n",
        "\n",
        "    def __str__(self):\n",
        "        \"\"\"Returns the string representation of the object for printing\n",
        "        :return: The string representation of the object\n",
        "        \"\"\"\n",
        "        ret_str = 'My index:' + str(self.my_idx) + '\\nOutput weights:'\n",
        "        for conn in self.output_weights:\n",
        "            ret_str += str(conn.weight) + ' '\n",
        "        return ret_str\n",
        "\n",
        "\n",
        "class Net:\n",
        "    def __init__(self, topology):\n",
        "        \"\"\"Constructor to create a neural net\n",
        "        :param topology: List that contains the number of neurons in each layer\n",
        "        \"\"\"\n",
        "        # Represent the layers in the neural net\n",
        "        self.layers = list()\n",
        "\n",
        "        # Each layer contains a list of neurons\n",
        "        for layer_num, neuron_count in enumerate(topology):\n",
        "            neuron_list = list()\n",
        "            num_outputs = 0 if layer_num == len(topology) - 1 else topology[layer_num + 1]\n",
        "            for index in range(neuron_count + 1):\n",
        "                neuron_list.append(Neuron(index, num_outputs))\n",
        "            self.layers.append(neuron_list)\n",
        "\n",
        "        self.error = 0.0\n",
        "\n",
        "    def feed_forward(self, input_vals):\n",
        "        \"\"\"Performs forward propagation in the network\n",
        "        :param input_vals: List of input parameters\n",
        "        \"\"\"\n",
        "        assert len(input_vals) == len(self.layers[0]) - 1\n",
        "\n",
        "        # Assigning input values to the input neurons\n",
        "        for index in range(len(input_vals)):\n",
        "            self.layers[0][index].output_val = input_vals[index]\n",
        "\n",
        "        # Forward propagation\n",
        "        for index in range(1, len(self.layers)):\n",
        "            prev_layer = self.layers[index - 1]\n",
        "            for neuron_idx in range(len(self.layers[index]) - 1):\n",
        "                self.layers[index][neuron_idx].feed_forward(prev_layer)\n",
        "\n",
        "    def back_prop(self, target_vals):\n",
        "        \"\"\"Performs back propagation in the network\n",
        "        :param target_vals: List of output values\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # Calculate overall net error (RMS of output neuron errors)\n",
        "        output_layer = self.layers[len(self.layers) - 1]\n",
        "        error = 0.0\n",
        "\n",
        "        for neuron_idx in range(len(target_vals)):\n",
        "            delta = target_vals[neuron_idx] - output_layer[neuron_idx].output_val\n",
        "            error += (delta ** 2)\n",
        "\n",
        "        error /= (len(target_vals))  # Average error squared\n",
        "        self.error = math.sqrt(error)  # RMS\n",
        "\n",
        "        # Calculate output layer gradients\n",
        "        for neuron_idx in range(0, len(output_layer) - 1):\n",
        "            output_layer[neuron_idx].calc_output_gradient(target_vals[neuron_idx])\n",
        "\n",
        "        # Calculate gradients on hidden layers\n",
        "        for layer_idx in range(len(self.layers) - 2, 0, -1):\n",
        "            hidden_layer = self.layers[layer_idx]\n",
        "            next_layer = self.layers[layer_idx + 1]\n",
        "\n",
        "            for hidden_neuron in hidden_layer:\n",
        "                hidden_neuron.calc_hidden_gradient(next_layer)\n",
        "\n",
        "        # For all layers from output to first hidden layer,\n",
        "        # update connection weights\n",
        "        for layer_idx in range(len(self.layers) - 1, 0, -1):\n",
        "            curr_layer = self.layers[layer_idx]\n",
        "            prev_layer = self.layers[layer_idx - 1]\n",
        "\n",
        "            for neuron_idx in range(0, len(curr_layer) - 1):\n",
        "                neuron = curr_layer[neuron_idx]\n",
        "                neuron.update_input_weights(prev_layer)\n",
        "\n",
        "    def get_results(self):\n",
        "        \"\"\"Returns the results\n",
        "        :return: An array of predicted output\n",
        "        \"\"\"\n",
        "        results = list()\n",
        "        output_layer = self.layers[len(self.layers) - 1]\n",
        "        for idx in range(0, len(output_layer) - 1):\n",
        "            results.append(output_layer[idx].output_val)\n",
        "        return results\n",
        "\n",
        "\n",
        "def build_model(input_shape):\n",
        "    \"\"\"\n",
        "    Arugments:\n",
        "        input_shape: A tuple representing the shape of the input of the model. shape=(image_width, image_height, #_channels)\n",
        "    Returns:\n",
        "        model: A Model object.\n",
        "    \"\"\"\n",
        "    # Define the input placeholder as a tensor with shape input_shape. \n",
        "    X_input = Input(input_shape) # shape=(?, 240, 240, 3)\n",
        "    \n",
        "    # Zero-Padding: pads the border of X_input with zeroes\n",
        "    X = ZeroPadding2D((2, 2))(X_input) # shape=(?, 244, 244, 3)\n",
        "    \n",
        "    # CONV -> BN -> RELU Block applied to X\n",
        "    X = Conv2D(32, (7, 7), strides = (1, 1), name = 'conv0')(X)\n",
        "    X = BatchNormalization(axis = 3, name = 'bn0')(X)\n",
        "    X = Activation('relu')(X) # shape=(?, 238, 238, 32)\n",
        "    \n",
        "    # MAXPOOL\n",
        "    X = MaxPooling2D((4, 4), name='max_pool0')(X) # shape=(?, 59, 59, 32) \n",
        "    \n",
        "    # MAXPOOL\n",
        "    X = MaxPooling2D((4, 4), name='max_pool1')(X) # shape=(?, 14, 14, 32)\n",
        "    \n",
        "    # FLATTEN X \n",
        "    X = Flatten()(X) # shape=(?, 6272)\n",
        "    # FULLYCONNECTED\n",
        "    X = Dense(1, activation='sigmoid', name='fc')(X) # shape=(?, 1)\n",
        "    \n",
        "    # Create model. This creates your Keras model instance, you'll use this instance to train/test the model.\n",
        "    model = Model(inputs = X_input, outputs = X, name='BrainDetectionModel')\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfEadAZVozh3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "e20a04fc-4736-4105-a02a-649585b4e9b8"
      },
      "source": [
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQMAAAEGCAYAAABhHPB4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}